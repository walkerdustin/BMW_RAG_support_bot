{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with retrieval\n",
    "\n",
    "## Stages of querying\n",
    "\n",
    "However, there is more to querying than initially meets the eye. Querying consists of three distinct stages:\n",
    "\n",
    "- **Retrieval** is when you find and return the most relevant documents for your query from your Index. As previously discussed in indexing, the most common type of retrieval is \"top-k\" semantic retrieval, but there are many other retrieval strategies.\n",
    "- **Postprocessing** is when the Nodes retrieved are optionally reranked, transformed, or filtered, for instance by requiring that they have specific metadata such as keywords attached.\n",
    "- **Response synthesis** is when your query, your most-relevant data and your prompt are combined and sent to your LLM to return a response.\n",
    "\n",
    "https://docs.llamaindex.ai/en/stable/examples/retrievers/ensemble_retrieval/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer, VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the encoding for GPT 3 and 4\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\", embed_batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild storage context\n",
    "storage_context_user_mauals = StorageContext.from_defaults(\n",
    "    persist_dir=\"data/embeddings/user_manuals_512_text_splitter_oAI_large_embed\"\n",
    ")\n",
    "# load index\n",
    "index_user_manuals = load_index_from_storage(storage_context_user_mauals)\n",
    "\n",
    "storage_context_forum_content = StorageContext.from_defaults(persist_dir=\"data/embeddings/512_text_splitter_oAI_large_embed\")\n",
    "# load index\n",
    "index_forum_content = load_index_from_storage(storage_context_forum_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever_user_manuals = VectorIndexRetriever(\n",
    "    index=index_user_manuals,\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "\n",
    "retriever_forum_content = VectorIndexRetriever(\n",
    "    index=index_forum_content,\n",
    "    similarity_top_k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"You are an expert for BMW Cars. You will get technical questions regarding the 3er and 4er series, or you will solve the Problem, the User has regarding their BMW Car. \\n\"\n",
    "    \"You will ground your answer based on Chunks of Information from the User Manual and the Forum Content to answer the User's Question. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Chunks of content from the User Manual (The information may be presented in a bad formatting with mistakes in the words): \\n\"\n",
    "    \"{user_maual_content}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Chunks of content from the Forum Content (This Information is based on content from a Car Forum, so the Information may be incorrect and presented with bad grammar): \\n\"\n",
    "    \"{forum_content}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    'Given this information, please answer the question or ask for the specific Information you need: \"\"\"{query_str}\"\"\"\\n'\n",
    ")\n",
    "prompt_tmpl = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make my own query Pipeline\n",
    "\n",
    "because the QueryPipeline of LlamaIndex is to confusing\n",
    "\n",
    "- first get the top 5 chunks of both indexes\n",
    "- create a prompt from the prompt template\n",
    "- generate answer with llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_string_from_nodes(nodes):\n",
    "    content = \"\"\n",
    "    for i, n in enumerate(nodes):\n",
    "        content += f\"<START_CHUNK_{i}>\"\n",
    "        content += n.node.text\n",
    "        content += f\"<END_CHUNK_{i}>\"\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the scores of the retrieved nodes are:\n",
      "0.572191879507313\n",
      "0.5596483450904651\n",
      "0.5537543356519133\n",
      "0.5518693217530177\n",
      "0.5470154969750757\n",
      "0.7149026234089527\n",
      "0.7017397502556473\n",
      "0.7001378485854379\n",
      "0.6964518343737596\n",
      "0.6934841027005193\n",
      "Total prompt length is 5160 tokens\n",
      "Based on the information provided, it seems like there could be multiple issues affecting the operation of your manual transmission in your 1996 320i e36. Here are some insights based on the chunks of information:\n",
      "\n",
      "1. **Clutch Malfunction**: The golden dust found in the gear oil could indeed be coming from the synchronizer rings, indicating potential wear. The feeling that the clutch is not declutching properly could be a sign of clutch issues. To test the clutch, you can perform a simple test by engaging the clutch and trying to shift gears smoothly. If there is resistance or grinding, it could indicate a problem with the clutch.\n",
      "\n",
      "2. **Clutch Engagement**: The fact that you might have driven without fully disengaging the clutch in the past could have contributed to the clutch wear and the presence of golden dust. This improper use of the clutch can lead to premature wear and malfunction.\n",
      "\n",
      "3. **Shaking While Starting**: The shaking you experience when starting the car could be related to the clutch not fully disengaging, causing some roughness in the engagement of the gears.\n",
      "\n",
      "4. **Differential Noise**: The bump sound from the differential when engaging the first gear could be a sign of issues with the differential itself. It's not normal to hear such sounds, and it could indicate a problem that needs to be addressed.\n",
      "\n",
      "Given these symptoms, it would be advisable to have a professional mechanic inspect the clutch system, synchronizer rings, and differential to diagnose the exact cause of the issues you are experiencing. Proper diagnosis and repair can help ensure the longevity and smooth operation of your manual transmission.\n"
     ]
    }
   ],
   "source": [
    "# make my own\n",
    "user_query = \"\"\"# Trouble with manual transmission\n",
    "\n",
    "## Question of username#2110\n",
    "\n",
    "Posted on 2023-03-15 12:58:00-04:00\n",
    "\n",
    "Hi everyone, my 1996 320i e36 manual transmission is slightly hard to operate\n",
    "and i recently found some golden dust in the gear oil (atf dexron II + bardhal\n",
    "t&d), probably coming from the synchronizer rings.  \n",
    "I have the feeling it might be the clutch not declutching properly.  \n",
    "How can i test it?  \n",
    "Opinions?  \n",
    "What could cause the clutch malfunction?  \n",
    "In the past I renewed the input and output cylinders, but for some time,\n",
    "before realizing the failure, I might have drive not desengaging totally the\n",
    "clutch, could the dust be coming from that period?  \n",
    "Another minor malfunction appears while starting the car, i mean when I start\n",
    "moving, it shakes for a little while.  \n",
    "I also noticed that when I'm still, engine running, when engaging the first\n",
    "gear i can hear a little bump from the differencial, kinda motorbike like...\n",
    "it's always been like that, Is that normal?\"\"\"\n",
    "\n",
    "user_maunal_nodes = retriever_user_manuals.retrieve(user_query)\n",
    "forum_content_nodes = retriever_forum_content.retrieve(user_query)\n",
    "\n",
    "# you can create text prompt (for completion API)\n",
    "prompt = prompt_tmpl.format(\n",
    "    user_maual_content=create_string_from_nodes(user_maunal_nodes),\n",
    "    forum_content=create_string_from_nodes(forum_content_nodes),\n",
    "    query_str=user_query,\n",
    ")\n",
    "print(\"the scores of the retrieved nodes are:\")\n",
    "for node in user_maunal_nodes:\n",
    "    print(node.score)\n",
    "for node in forum_content_nodes:\n",
    "    print(node.score)\n",
    "print(f\"Total prompt length is {len(enc.encode(prompt))} tokens\")\n",
    "res = llm.complete(prompt)\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus, I bought the right fluid and filled it into the right reservoir,\n",
      "correct? (Please see attached image).  \n",
      "Please correct me if I'm wrong.  \n",
      "  \n",
      "Thank you in advance for trying to help, 82Eye!\n",
      "\n",
      "### Post of username#6\n",
      "\n",
      "  \n",
      "it's good to make sure it is filled but it has nothing to do with either the\n",
      "clutch or the transmission.\n",
      "\n",
      "### Post of username#513\n",
      "\n",
      "Thanks for your quick response, 82eye!  \n",
      "Thus, what could be the cause of the problem of hard to shift gears after the\n",
      "engine is started?\n",
      "\n",
      "### Post of username#6\n",
      "\n",
      "check your brake res off the brake booster. it takes the dot 4. the brakes and\n",
      "clutch share a res. it's possible you are low on fluid there, but more likely\n",
      "you are in need of a new clutch.  \n",
      "  \n",
      "the trans oil is filled from underneath the car, there are two plugs in the\n",
      "side of the trans to drain and fill. i would expect more trouble than hard\n",
      "shifting if it's low.\n",
      "\n",
      "### Post of username#1616\n",
      "\n",
      "Clutch pilot bearing is siezed/bound up and dragging so input shaft is not\n",
      "disengaging fully or a broken spring in clutch cover causing clutch to drag.  \n",
      "  \n",
      "Sent from my SM-G960U using Tapatalk\n",
      "\n",
      "### Post of username#6\n",
      "\n",
      "kind of figure something the same. it's a clutch job at the end of the day no\n",
      "matter.\n",
      "\n",
      "### Post of username#513\n",
      "\n",
      "I checked the brake fluid level, and it was fine. When I started the car, it\n",
      "sounded louder (but not much louder) and heavier than usual. Sounded like a\n",
      "grinding sound which came from underneath the car/engine. What could have this\n",
      "indicated?  \n",
      "  \n",
      "Thank you!\n",
      "\n",
      "### Post of username#6\n",
      "\n",
      "your clutch is shot. the clutch is built in two pieces / faces and is sprung\n",
      "in the middle. it sounds like your clutch lost a spring and it's now chewing\n",
      "up other stuff inside your transmission.  \n",
      "  \n",
      "i would do a clutch job now before you wind up destroying the trans. a clutch\n",
      "will be cheaper than trying to source a trans if you pooch it.\n"
     ]
    }
   ],
   "source": [
    "print(forum_content_nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try out the QueryPipeline\n",
    "I was not able to make this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_qa_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: '), conditionals=[(<function is_chat_model at 0x000001A81F732F20>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content=\"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\", additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: ', additional_kwargs={})]))]),\n",
       " 'refine_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template=\"The original query is as follows: {query_str}\\nWe have provided an existing answer: {existing_answer}\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\n{context_msg}\\n------------\\nGiven the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\nRefined Answer: \"), conditionals=[(<function is_chat_model at 0x000001A81F732F20>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_msg', 'query_str', 'existing_answer'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.USER: 'user'>, content=\"You are an expert Q&A system that strictly operates in two modes when refining existing answers:\\n1. **Rewrite** an original answer using the new context.\\n2. **Repeat** the original answer if the new context isn't useful.\\nNever reference the original answer or context directly in your answer.\\nWhen in doubt, just repeat the original answer.\\nNew Context: {context_msg}\\nQuery: {query_str}\\nOriginal Answer: {existing_answer}\\nNew Answer: \", additional_kwargs={})]))])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "response_synthesizer.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(response_synthesizer.get_prompts()[\"text_qa_template\"].get_template())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    }
   ],
   "source": [
    "print(response_synthesizer.get_prompts()[\"refine_template\"].get_template())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever_user_manuals,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Query Pipeline\n",
    "\n",
    "To chain together two different indexes and to customize the prompts a Custom Query Pipeline is used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"We have provided context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this information, please answer the question: {query_str}\\n\"\n",
    ")\n",
    "prompt_tmpl = PromptTemplate(template)\n",
    "\n",
    "# you can create text prompt (for completion API)\n",
    "prompt = prompt_tmpl.format(context_str=..., query_str=...)\n",
    "\n",
    "# or easily convert to message prompts (for chat API)\n",
    "messages = prompt_tmpl.format_messages(context_str=..., query_str=...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (2256392507.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[38], line 14\u001b[1;36m\u001b[0m\n\u001b[1;33m    p.run(prompt_key1=\"<input1>\", ...)\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# sequential chain\n",
    "p = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)\n",
    "\n",
    "# DAG\n",
    "p = QueryPipeline(verbose=True)\n",
    "p.add_modules({\"prompt_tmpl\": prompt_tmpl, \"llm\": llm})\n",
    "p.add_link(\"prompt_tmpl\", \"llm\")\n",
    "\n",
    "# run pipeline\n",
    "p.run(prompt_key1=\"<input1>\", ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (4192935487.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[20], line 13\u001b[1;36m\u001b[0m\n\u001b[1;33m    p.run(prompt_key1=\"<input1>\", ...)\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# sequential chain\n",
    "p = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)\n",
    "\n",
    "# DAG\n",
    "p = QueryPipeline(verbose=True)\n",
    "p.add_modules({\"prompt_tmpl\": prompt_tmpl, \"llm\": llm})\n",
    "p.add_link(\"prompt_tmpl\", \"llm\")\n",
    "\n",
    "# run pipeline\n",
    "p.run(prompt_key1=\"<input1>\", ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# define modules\n",
    "template = (\n",
    "    \"We have provided context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this information, please answer the question: {query_str}\\n\"\n",
    ")\n",
    "prompt_tmpl = PromptTemplate(template)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "# define query pipeline\n",
    "p = QueryPipeline(verbose=True)\n",
    "p.add_modules(\n",
    "    {\n",
    "        \"retriever_forum_content\": retriever_forum_content,\n",
    "        \"prompt_tmpl\": prompt_tmpl,\n",
    "        \"llm\": llm,\n",
    "    }\n",
    ")\n",
    "p.add_link(\"retriever_forum_content\", \"prompt_tmpl\", dest_key=\"context_str\")\n",
    "p.add_link(\"prompt_tmpl\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "required_keys={'input'} optional_keys=set()\n",
      "required_keys={'output'}\n",
      "required_keys={'query_str', 'context_str'} optional_keys=set()\n",
      "required_keys={'prompt'}\n",
      "required_keys={'messages'} optional_keys=set()\n",
      "required_keys={'output'}\n"
     ]
    }
   ],
   "source": [
    "print(retriever_forum_content.as_query_component().input_keys)\n",
    "print(retriever_forum_content.as_query_component().output_keys)\n",
    "print(prompt_tmpl.as_query_component().input_keys)\n",
    "print(prompt_tmpl.as_query_component().output_keys)\n",
    "print(llm.as_query_component().input_keys)\n",
    "print(llm.as_query_component().output_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module retriever_forum_content with input: \n",
      "input: The alternator of my E36 is not working. What should I do?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module prompt_tmpl with input: \n",
      "context_str: [NodeWithScore(node=TextNode(id_='612e444b-c2b2-4dd2-ba79-ce08884598f6', embedding=None, metadata={'thread_id': '2477166', 'category': '14-1991-1999-(E36)', 'thread_title': 'Replaced my alternator and...\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Required keys {'query_str', 'context_str'} are not present in input keys {'context_str'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe alternator of my E36 is not working. What should I do?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Git_Repos\\BMW_RAG_support_bot\\venv\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:319\u001b[0m, in \u001b[0;36mQueryPipeline.run\u001b[1;34m(self, return_values_direct, callback_manager, *args, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     query_payload \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mstr\u001b[39m(kwargs))\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    317\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_payload}\n\u001b[0;32m    318\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_values_direct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_values_direct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Git_Repos\\BMW_RAG_support_bot\\venv\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:442\u001b[0m, in \u001b[0;36mQueryPipeline._run\u001b[1;34m(self, return_values_direct, *args, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m root_key, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_root_key_and_kwargs(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;66;03m# call run_multi with one root key\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m result_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_multi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mroot_key\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_single_result_output(result_outputs, return_values_direct)\n",
      "File \u001b[1;32mc:\\Git_Repos\\BMW_RAG_support_bot\\venv\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:544\u001b[0m, in \u001b[0;36mQueryPipeline._run_multi\u001b[1;34m(self, module_input_dict)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    543\u001b[0m     print_debug_input(module_key, module_input)\n\u001b[1;32m--> 544\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_component\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;66;03m# get new nodes and is_leaf\u001b[39;00m\n\u001b[0;32m    547\u001b[0m queue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_component_output(\n\u001b[0;32m    548\u001b[0m     queue, output_dict, module_key, all_module_inputs, result_outputs\n\u001b[0;32m    549\u001b[0m )\n",
      "File \u001b[1;32mc:\\Git_Repos\\BMW_RAG_support_bot\\venv\\Lib\\site-packages\\llama_index\\core\\base\\query_pipeline\\query.py:198\u001b[0m, in \u001b[0;36mQueryComponent.run_component\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run component.\"\"\"\u001b[39;00m\n\u001b[0;32m    197\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_dict)\n\u001b[1;32m--> 198\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_component_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m component_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_component(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_component_outputs(component_outputs)\n",
      "File \u001b[1;32mc:\\Git_Repos\\BMW_RAG_support_bot\\venv\\Lib\\site-packages\\llama_index\\core\\base\\query_pipeline\\query.py:186\u001b[0m, in \u001b[0;36mQueryComponent.validate_component_inputs\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Validate component inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# make sure set of input keys == self.input_keys\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_keys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_component_inputs(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[1;32mc:\\Git_Repos\\BMW_RAG_support_bot\\venv\\Lib\\site-packages\\llama_index\\core\\base\\query_pipeline\\query.py:84\u001b[0m, in \u001b[0;36mInputKeys.validate\u001b[1;34m(self, input_keys)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# check if required keys are present, and that keys all are in required or optional\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequired_keys\u001b[38;5;241m.\u001b[39missubset(input_keys):\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequired keys \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequired_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are not present in input keys \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m     )\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m input_keys\u001b[38;5;241m.\u001b[39missubset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequired_keys\u001b[38;5;241m.\u001b[39munion(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptional_keys)):\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput keys \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m contain keys not in required or optional keys \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequired_keys\u001b[38;5;241m.\u001b[39munion(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptional_keys)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Required keys {'query_str', 'context_str'} are not present in input keys {'context_str'}"
     ]
    }
   ],
   "source": [
    "p.run(input=\"The alternator of my E36 is not working. What should I do?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
